{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install depedencies"
      ],
      "metadata": {
        "id": "hrhNp6xvHEN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T23:22:53.123284Z",
          "iopub.status.busy": "2024-05-29T23:22:53.122418Z",
          "iopub.status.idle": "2024-05-29T23:23:04.885908Z",
          "shell.execute_reply": "2024-05-29T23:23:04.884787Z",
          "shell.execute_reply.started": "2024-05-29T23:22:53.123284Z"
        },
        "id": "YU6t2zeXGVQ4"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_msssim ripser scikit-tda pytorch-wavelets\n",
        "!pip install -U --no-cache-dir gdown --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download data"
      ],
      "metadata": {
        "id": "irEOJCGSHTsT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T22:20:09.374023Z",
          "iopub.status.busy": "2024-05-29T22:20:09.373664Z",
          "iopub.status.idle": "2024-05-29T22:21:48.858048Z",
          "shell.execute_reply": "2024-05-29T22:21:48.856287Z",
          "shell.execute_reply.started": "2024-05-29T22:20:09.373995Z"
        },
        "id": "pPD1YKPJGVQ6"
      },
      "outputs": [],
      "source": [
        "!gdown '14yLiI7R8ghl0BlAGMSgMEuafA9t9Whxa' -O 'SD_Astro_combined.h5'\n",
        "!gdown '1x0B4vog2sqJ4P-kJOKpYupuR0hwkbTMs' -O 'SD_Astro4.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract h5 image files to a folder, go through each sd combined(34000 images) astro4 dataset (+11000)"
      ],
      "metadata": {
        "id": "_7RswJRgHXTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-05-29T22:21:48.860223Z",
          "iopub.status.busy": "2024-05-29T22:21:48.859965Z",
          "iopub.status.idle": "2024-05-29T22:28:19.208847Z",
          "shell.execute_reply": "2024-05-29T22:28:19.207739Z",
          "shell.execute_reply.started": "2024-05-29T22:21:48.860196Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "aa8eVNxCGVQ6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "def save_image(image_data, idx, output_dir):\n",
        "    img = Image.fromarray(image_data.astype('uint8'), 'RGB')\n",
        "    img.save(os.path.join(output_dir, f'image_{idx:05d}.png'))\n",
        "\n",
        "def extract_images_chunk(h5_file, output_dir, start_idx, end_idx, initial_idx):\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        images = np.array(f['images'][start_idx:end_idx])\n",
        "        for idx, image in enumerate(images):\n",
        "            save_image(image, initial_idx + start_idx + idx, output_dir)\n",
        "            print(f\"Saved image {initial_idx + start_idx + idx}\")\n",
        "\n",
        "def extract_images_parallel(h5_file, output_dir, chunk_size=1000):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Find the highest existing image index\n",
        "    existing_images = [int(f.split('_')[1].split('.')[0]) for f in os.listdir(output_dir) if f.endswith('.png')]\n",
        "    if existing_images:\n",
        "        initial_idx = max(existing_images) + 1\n",
        "    else:\n",
        "        initial_idx = 0\n",
        "\n",
        "    total_images_before = len(existing_images)\n",
        "    print(f\"Total images before adding new images: {total_images_before}\")\n",
        "\n",
        "    with h5py.File(h5_file, 'r') as f:\n",
        "        num_images = f['images'].shape[0]\n",
        "\n",
        "    pool = Pool(cpu_count())\n",
        "    chunks = [(start_idx, min(start_idx + chunk_size, num_images)) for start_idx in range(0, num_images, chunk_size)]\n",
        "\n",
        "    for start_idx, end_idx in chunks:\n",
        "        pool.apply_async(extract_images_chunk, (h5_file, output_dir, start_idx, end_idx, initial_idx))\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # Count the total images after adding the new images\n",
        "    total_images_after = len([f for f in os.listdir(output_dir) if f.endswith('.png')])\n",
        "    print(f\"Total images after adding new images: {total_images_after}\")\n",
        "\n",
        "# Path to the HDF5 file and output directory\n",
        "sd_astro_path = \"SD_Astro_combined.h5\"\n",
        "output_dir = \"Astro_images\"\n",
        "\n",
        "# Extract images using multiprocessing\n",
        "extract_images_parallel(sd_astro_path, output_dir)\n",
        "print(f\"Images extracted to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset and dataloader"
      ],
      "metadata": {
        "id": "VavwiByqHtXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T23:23:04.888319Z",
          "iopub.status.busy": "2024-05-29T23:23:04.887917Z",
          "iopub.status.idle": "2024-05-29T23:23:10.955686Z",
          "shell.execute_reply": "2024-05-29T23:23:10.954650Z",
          "shell.execute_reply.started": "2024-05-29T23:23:04.888281Z"
        },
        "id": "Djd0v6FEGVQ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, random_crop_size=256):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "        self.random_crop = transforms.RandomCrop(random_crop_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        if self.random_crop:\n",
        "            image = self.random_crop(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "image_dir = \"Astro_images\"\n",
        "image_dataset = ImageFolderDataset(image_dir, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.95 * len(image_dataset))\n",
        "val_size = len(image_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(image_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 6  # Adjusted for efficiency\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=7, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialize networks"
      ],
      "metadata": {
        "id": "gfQSYMQtH0YR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T23:23:10.961450Z",
          "iopub.status.busy": "2024-05-29T23:23:10.960984Z",
          "iopub.status.idle": "2024-05-29T23:23:12.823994Z",
          "shell.execute_reply": "2024-05-29T23:23:12.822031Z",
          "shell.execute_reply.started": "2024-05-29T23:23:10.961424Z"
        },
        "id": "6FS5bz3A3yfe"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_\n",
        "from timm.models.vision_transformer import Block\n",
        "import pytorch_wavelets\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers=4):\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_ch = in_channels + i * growth_rate\n",
        "            out_ch = growth_rate if i < num_layers - 1 else in_channels\n",
        "            self.layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = [x]\n",
        "        for layer in self.layers:\n",
        "            out = self.relu(layer(torch.cat(inputs, dim=1)))\n",
        "            inputs.append(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, width, height = x.size()\n",
        "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, channels, width, height)\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(CBAMBlock, self).__init__()\n",
        "        self.channel_attention = ChannelAttentionBlock(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttentionBlock()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.channel_attention(x)\n",
        "        x_out = self.spatial_attention(x_out)\n",
        "        return x_out\n",
        "\n",
        "class ChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttentionBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.fc(y)\n",
        "        return x * y\n",
        "\n",
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttentionBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_out = torch.cat([avg_out, max_out], dim=1)\n",
        "        x_out = self.conv(x_out)\n",
        "        return x * self.sigmoid(x_out)\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_blocks=3, num_layers=4):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.blocks = nn.ModuleList([ResidualDenseBlock(in_channels, growth_rate, num_layers) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class DynamicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DynamicConv2d, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding)\n",
        "\n",
        "class WaveletTransform(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveletTransform, self).__init__()\n",
        "        self.dwt = pytorch_wavelets.DWTForward(J=1, wave='haar', mode='zero')\n",
        "        self.iwt = pytorch_wavelets.DWTInverse(wave='haar', mode='zero')\n",
        "\n",
        "    def forward(self, x):\n",
        "        yl, yh = self.dwt(x)\n",
        "        recon = self.iwt((yl, yh))\n",
        "        return recon\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = in_channels // 2\n",
        "\n",
        "        self.g = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.theta = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.phi = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.W = nn.Conv2d(self.inter_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        nn.init.constant_(self.W.weight, 0)\n",
        "        nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        f_div_C = F.softmax(f, dim=-1)\n",
        "\n",
        "        y = torch.matmul(f_div_C, g_x)\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "        y = y.view(batch_size, self.inter_channels, h, w)\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        return z\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(self._make_layer(in_channels + i * growth_rate, growth_rate))\n",
        "\n",
        "    def _make_layer(self, in_channels, growth_rate):\n",
        "        layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(growth_rate),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_features = layer(torch.cat(features, dim=1))\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "class PixelShuffleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, upscale_factor):\n",
        "        super(PixelShuffleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class MultiScaleFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(MultiScaleFeatureExtractor, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=1, padding=3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.non_local = NonLocalBlock(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.relu(self.bn1(self.conv1(x)))\n",
        "        out2 = self.relu(self.bn2(self.conv2(x)))\n",
        "        out3 = self.relu(self.bn3(self.conv3(x)))\n",
        "        multi_scale_features = out1 + out2 + out3\n",
        "        return self.non_local(multi_scale_features)\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, depths, num_heads, window_size=7):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depths = depths\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio=4., qkv_bias=True)\n",
        "            for _ in range(depths)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, channels, height, width)\n",
        "        return x\n",
        "\n",
        "class ImprovedDenoisingNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ImprovedDenoisingNetwork, self).__init__()\n",
        "        self.initial_conv = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            DenseBlock(64, 32, num_layers=4),\n",
        "            ResidualBlock(192, 128, stride=2),  # DenseBlock output channels + 64 initial channels\n",
        "            ResidualBlock(128, 128, stride=2)\n",
        "        )\n",
        "\n",
        "        self.multi_scale = MultiScaleFeatureExtractor(128)\n",
        "        self.transformer = SwinTransformerBlock(embed_dim=128, depths=2, num_heads=4)\n",
        "\n",
        "        self.rir_block1 = ResidualInResidualDenseBlock(128, 32, num_blocks=2, num_layers=3)\n",
        "        self.attention1 = CBAMBlock(128)\n",
        "\n",
        "        self.rir_block2 = ResidualInResidualDenseBlock(128, 32, num_blocks=2, num_layers=3)\n",
        "        self.attention2 = CBAMBlock(128)\n",
        "\n",
        "        self.dynamic_conv = DynamicConv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.wavelet_transform = WaveletTransform()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            PixelShuffleBlock(128, 64, upscale_factor=2),\n",
        "            PixelShuffleBlock(64, 64, upscale_factor=2),\n",
        "            nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        encoded = self.encoder(x)\n",
        "        multi_scale_features = self.multi_scale(encoded)\n",
        "        transformer_features = self.transformer(multi_scale_features)\n",
        "\n",
        "        rir1 = self.rir_block1(transformer_features)\n",
        "        att1 = self.attention1(rir1)\n",
        "\n",
        "        rir2 = self.rir_block2(att1)\n",
        "        att2 = self.attention2(rir2)\n",
        "\n",
        "        dynamic_conv_features = self.dynamic_conv(att2)\n",
        "        wavelet_features = self.wavelet_transform(dynamic_conv_features)\n",
        "\n",
        "        decoded = self.decoder(wavelet_features)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DigJ8CeqGVQ8"
      },
      "source": [
        "noise augments, Loss functions.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T23:23:12.826299Z",
          "iopub.status.busy": "2024-05-29T23:23:12.825801Z",
          "iopub.status.idle": "2024-05-29T23:23:14.122452Z",
          "shell.execute_reply": "2024-05-29T23:23:14.121440Z",
          "shell.execute_reply.started": "2024-05-29T23:23:12.826299Z"
        },
        "id": "AkAAv981SQA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import models\n",
        "from torch.fft import fft2, ifft2\n",
        "from pytorch_msssim import ms_ssim\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.feature import canny\n",
        "from skimage.color import rgb2lab, deltaE_ciede2000, rgb2yuv\n",
        "\n",
        "# Function to scale Gaussian noise to emulate different sensor resolutions\n",
        "def scale_gaussian_noise(noise_tensor, scale_factor):\n",
        "    batch_size, channels, height, width = noise_tensor.shape\n",
        "    scaled_noise = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            noise_np = noise_tensor[i, c].cpu().numpy()\n",
        "\n",
        "            # Resize the noise to emulate higher/lower resolution\n",
        "            scaled_noise_np = cv2.resize(noise_np, (int(width * scale_factor), int(height * scale_factor)), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Resize it back to the original size\n",
        "            scaled_noise_np = cv2.resize(scaled_noise_np, (width, height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Convert back to tensor\n",
        "            scaled_noise_tensor = torch.tensor(scaled_noise_np).to(noise_tensor.device)\n",
        "\n",
        "            # Append scaled noise to the list\n",
        "            if c == 0:\n",
        "                scaled_noise_batch = scaled_noise_tensor.unsqueeze(0)\n",
        "            else:\n",
        "                scaled_noise_batch = torch.cat((scaled_noise_batch, scaled_noise_tensor.unsqueeze(0)), 0)\n",
        "\n",
        "        scaled_noise.append(scaled_noise_batch.unsqueeze(0))\n",
        "\n",
        "    scaled_noise_tensor = torch.cat(scaled_noise, 0)\n",
        "    return scaled_noise_tensor\n",
        "\n",
        "# Define the function to apply rotational Gaussian noise with scaling\n",
        "def apply_rotational_gaussian_noise(image_tensor, std=20, mean=0, num_frames=40):\n",
        "    batch_size, channels, height, width = image_tensor.shape\n",
        "\n",
        "    # Randomly determine the center point once at the start\n",
        "    center_x = np.random.randint(0, width)\n",
        "    center_y = np.random.randint(0, height)\n",
        "    center = (center_x, center_y)\n",
        "\n",
        "    # Randomly vary the standard deviation at the start\n",
        "    std_variation = np.random.uniform(0.5, 2.5) * std\n",
        "\n",
        "    # Generate noise using torch.randn\n",
        "    noise_tensor = torch.randn_like(image_tensor) * std_variation + mean\n",
        "\n",
        "    # Randomly select a scale factor between 0.5 and 1.5\n",
        "    scale_factor = np.random.uniform(0.5, 4)\n",
        "\n",
        "    # Scale the Gaussian noise\n",
        "    scaled_noise_tensor = scale_gaussian_noise(noise_tensor, scale_factor)\n",
        "\n",
        "    noisy_images = []\n",
        "    rotation_factor = np.random.uniform(1, 2)\n",
        "    for _ in range(num_frames):\n",
        "\n",
        "        angle = np.random.uniform(0, rotation_factor)\n",
        "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "\n",
        "        # Apply rotation to each image in the batch\n",
        "        rotated_noise_batch = []\n",
        "        for i in range(batch_size):\n",
        "            noise_np = scaled_noise_tensor[i].permute(1, 2, 0).cpu().numpy()\n",
        "            rotated_noise_np = cv2.warpAffine(noise_np, rotation_matrix, (width, height))\n",
        "            rotated_noise_tensor = torch.tensor(rotated_noise_np).permute(2, 0, 1).to(image_tensor.device)\n",
        "            rotated_noise_batch.append(rotated_noise_tensor)\n",
        "\n",
        "        rotated_noise_batch = torch.stack(rotated_noise_batch)\n",
        "\n",
        "        # Add rotated noise to image\n",
        "        noisy_image_tensor = image_tensor + rotated_noise_batch / 255.0\n",
        "        noisy_images.append(noisy_image_tensor)\n",
        "\n",
        "    # Stack and average the noisy images\n",
        "    stacked_image_tensor = torch.stack(noisy_images).mean(dim=0)\n",
        "\n",
        "    # Clip values to the range [0, 1]\n",
        "    stacked_image_tensor = torch.clamp(stacked_image_tensor, 0, 1)\n",
        "\n",
        "    return stacked_image_tensor\n",
        "\n",
        "# Define the function to add combined noise\n",
        "def add_combined_noise(image, mean=0, std=0.05, noise_type=None, sp_prob=0.002):\n",
        "    # Randomly select a scale factor between 0.5 and 1.5\n",
        "    scale_factor = np.random.uniform(0.5, 3.5)\n",
        "\n",
        "    if noise_type == 'rotational':\n",
        "        frames = int(np.random.uniform(40, 80))\n",
        "        noisy_image = apply_rotational_gaussian_noise(image, std=std*255, mean=mean*255, num_frames=frames)\n",
        "    elif noise_type == 'poisson':\n",
        "        multi = np.random.uniform(14, 25)\n",
        "        lambda_ = image * multi  # Adjust the multiplier to control intensity\n",
        "        lambda_ = torch.clamp(lambda_, min=0)  # Ensure lambda is non-negative\n",
        "        poisson_noise = torch.poisson(lambda_) / multi - image\n",
        "\n",
        "        noisy_image = image + poisson_noise\n",
        "    else:\n",
        "        # Randomly vary the standard deviation at the start\n",
        "        std_variation = np.random.uniform(0.55, 1.75) * std\n",
        "\n",
        "        # Apply Gaussian noise\n",
        "        gaussian_noise = torch.randn(image.size(), device=image.device) * std_variation + mean\n",
        "\n",
        "        # Scale the Gaussian noise\n",
        "        gaussian_noise = scale_gaussian_noise(gaussian_noise, scale_factor)\n",
        "\n",
        "        noisy_image = image + gaussian_noise\n",
        "\n",
        "        # Clamp values to ensure non-negativity\n",
        "        noisy_image = torch.clamp(noisy_image, 0., None)\n",
        "\n",
        "        # Optionally apply other types of noise\n",
        "        if noise_type == 'salt_and_pepper':\n",
        "            sp_noise = torch.rand(image.size(), device=image.device)\n",
        "            salt_pepper_noise = torch.where(sp_noise < sp_prob / 2, torch.ones_like(image), torch.zeros_like(image))\n",
        "            salt_pepper_noise = torch.where(sp_noise > 1 - sp_prob / 2, -torch.ones_like(image), salt_pepper_noise)\n",
        "\n",
        "            # Scale the salt and pepper noise\n",
        "            salt_pepper_noise = scale_gaussian_noise(salt_pepper_noise, scale_factor)\n",
        "\n",
        "            noisy_image += salt_pepper_noise\n",
        "\n",
        "    noisy_image = torch.clamp(noisy_image, 0., 1.)\n",
        "    return noisy_image\n",
        "\n",
        "from skimage.color import rgb2lab, rgb2yuv\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features\n",
        "        self.layers = [2, 7, 12, 21, 30]  # Conv1_2, Conv2_2, Conv3_4, Conv4_4, Conv5_4\n",
        "        self.model = nn.Sequential(*[vgg[i] for i in range(max(self.layers) + 1)])\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        loss = 0\n",
        "        for i in self.layers:\n",
        "            denoised_features = self.model[:i + 1](denoised)\n",
        "            target_features = self.model[:i + 1](target)\n",
        "            loss += F.l1_loss(denoised_features, target_features)\n",
        "        return loss\n",
        "\n",
        "class MSSSIMLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSSSIMLoss, self).__init__()\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        return 1 - ms_ssim(denoised, target, data_range=1.0, size_average=True)\n",
        "\n",
        "class FrequencyDomainLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FrequencyDomainLoss, self).__init__()\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        denoised_fft = torch.fft.fft2(denoised)\n",
        "        target_fft = torch.fft.fft2(target)\n",
        "        return F.l1_loss(torch.abs(denoised_fft), torch.abs(target_fft))\n",
        "\n",
        "class EnhancedEdgeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedEdgeLoss, self).__init__()\n",
        "        self.sobel_x = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=False, groups=3)\n",
        "        self.sobel_y = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=False, groups=3)\n",
        "        sobel_kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
        "        sobel_kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
        "        self.sobel_x.weight.data = sobel_kernel_x\n",
        "        self.sobel_y.weight.data = sobel_kernel_y\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        denoised_edges_x = self.sobel_x(denoised)\n",
        "        denoised_edges_y = self.sobel_y(denoised)\n",
        "        target_edges_x = self.sobel_x(target)\n",
        "        target_edges_y = self.sobel_y(target)\n",
        "        loss_x = F.l1_loss(denoised_edges_x, target_edges_x)\n",
        "        loss_y = F.l1_loss(denoised_edges_y, target_edges_y)\n",
        "\n",
        "        # Multi-scale edge loss\n",
        "        denoised_edges_x2 = self.sobel_x(F.interpolate(denoised, scale_factor=0.5, mode='bilinear'))\n",
        "        denoised_edges_y2 = self.sobel_y(F.interpolate(denoised, scale_factor=0.5, mode='bilinear'))\n",
        "        target_edges_x2 = self.sobel_x(F.interpolate(target, scale_factor=0.5, mode='bilinear'))\n",
        "        target_edges_y2 = self.sobel_y(F.interpolate(target, scale_factor=0.5, mode='bilinear'))\n",
        "        loss_x2 = F.l1_loss(denoised_edges_x2, target_edges_x2)\n",
        "        loss_y2 = F.l1_loss(denoised_edges_y2, target_edges_y2)\n",
        "\n",
        "        return loss_x + loss_y + 0.5 * (loss_x2 + loss_y2)\n",
        "\n",
        "def rgb_to_lab_tensor(image):\n",
        "    image = image.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "    lab = rgb2lab(image)\n",
        "    # Normalize LAB values: L [0, 100], A and B [-128, 127]\n",
        "    lab[:, :, :, 0] = lab[:, :, :, 0] / 100.0  # Normalize L to [0, 1]\n",
        "    lab[:, :, :, 1:] = (lab[:, :, :, 1:] + 128) / 255.0  # Normalize A, B to [0, 1]\n",
        "    return torch.from_numpy(lab).permute(0, 3, 1, 2)\n",
        "\n",
        "def rgb_to_yuv_tensor(image):\n",
        "    image = image.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "    yuv = rgb2yuv(image)\n",
        "    # Normalize YUV values: Y [0, 255], U and V [0, 255]\n",
        "    yuv[:, :, :, 0] = yuv[:, :, :, 0] / 255.0  # Normalize Y to [0, 1]\n",
        "    yuv[:, :, :, 1:] = (yuv[:, :, :, 1:] + 0.5)  # Normalize U, V to [0, 1]\n",
        "    return torch.from_numpy(yuv).permute(0, 3, 1, 2)\n",
        "\n",
        "class EnhancedColorLuminanceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedColorLuminanceLoss, self).__init__()\n",
        "\n",
        "    def colorfulness_metric(self, image):\n",
        "        rg = image[:, 0, :, :] - image[:, 1, :, :]\n",
        "        yb = 0.5 * (image[:, 0, :, :] + image[:, 1, :, :]) - image[:, 2, :, :]\n",
        "        rg_std, rg_mean = torch.std_mean(rg)\n",
        "        yb_std, yb_mean = torch.std_mean(yb)\n",
        "        return torch.sqrt(rg_std ** 2 + yb_std ** 2) + 0.3 * torch.sqrt(rg_mean ** 2 + yb_mean ** 2)\n",
        "\n",
        "    def dynamic_range_loss(self, denoised, target):\n",
        "        return F.l1_loss(torch.max(denoised) - torch.min(denoised), torch.max(target) - torch.min(target))\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        # LAB color space loss\n",
        "        denoised_lab = rgb_to_lab_tensor(denoised)\n",
        "        target_lab = rgb_to_lab_tensor(target)\n",
        "\n",
        "        delta_e = torch.tensor(deltaE_ciede2000(denoised_lab.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                                                target_lab.permute(0, 2, 3, 1).cpu().numpy()))\n",
        "        delta_e = delta_e.to(denoised.device)\n",
        "        color_loss = torch.mean(delta_e)\n",
        "\n",
        "        # Luminance loss using Y channel from YUV color space\n",
        "        denoised_yuv = rgb_to_yuv_tensor(denoised)\n",
        "        target_yuv = rgb_to_yuv_tensor(target)\n",
        "        luminance_loss = F.l1_loss(denoised_yuv[:, 0, :, :], target_yuv[:, 0, :, :])\n",
        "\n",
        "        # Colorfulness metric loss\n",
        "        colorfulness_l = F.l1_loss(self.colorfulness_metric(denoised), self.colorfulness_metric(target))\n",
        "\n",
        "        # Dynamic range loss\n",
        "        dynamic_range_l = self.dynamic_range_loss(denoised, target)\n",
        "\n",
        "        return color_loss + 0.85 * luminance_loss + 0.5 * colorfulness_l + 0.5 * dynamic_range_l\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    def __init__(self, epsilon=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, denoised, target):\n",
        "        return torch.mean(torch.sqrt((denoised - target) ** 2 + self.epsilon ** 2))\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, weight=1e-6):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, denoised):\n",
        "        batch_size = denoised.size()[0]\n",
        "        h_tv = torch.pow(denoised[:,:,1:,:] - denoised[:,:,:-1,:], 2).sum()\n",
        "        w_tv = torch.pow(denoised[:,:,:,1:] - denoised[:,:,:,:-1], 2).sum()\n",
        "        return self.weight * 2 * (h_tv + w_tv) / batch_size\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.charbonnier_loss = CharbonnierLoss()\n",
        "        self.perceptual_loss = PerceptualLoss()\n",
        "        self.msssim_loss = MSSSIMLoss()\n",
        "        self.frequency_domain_loss = FrequencyDomainLoss()\n",
        "        self.edge_loss = EnhancedEdgeLoss()\n",
        "        self.color_luminance_loss = EnhancedColorLuminanceLoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, denoised, target, noisy_input):\n",
        "        charbonnier = self.charbonnier_loss(denoised, target)\n",
        "        perceptual = self.perceptual_loss(denoised, target) / 100\n",
        "        msssim = self.msssim_loss(denoised, target)\n",
        "        frequency = self.frequency_domain_loss(denoised, target) / 100\n",
        "        edge = self.edge_loss(denoised, target)\n",
        "        color_luminance = self.color_luminance_loss(denoised, target)\n",
        "        tv = self.tv_loss(denoised)\n",
        "\n",
        "        # Adjusting weights for improved balance\n",
        "        return charbonnier + 0.35 * perceptual + 0.25 * msssim + 0.1 * frequency + 0.1 * edge + 0.3 * color_luminance + 0.05 * tv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-24T07:17:54.514475Z",
          "iopub.status.busy": "2024-05-24T07:17:54.513696Z",
          "iopub.status.idle": "2024-05-24T07:17:57.006655Z",
          "shell.execute_reply": "2024-05-24T07:17:57.005474Z",
          "shell.execute_reply.started": "2024-05-24T07:17:54.514437Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "id": "LGeQDYcuGVQ-"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from torch.fft import fft2, ifft2\n",
        "from pytorch_msssim import ssim\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Function to scale Gaussian noise to emulate different sensor resolutions\n",
        "def scale_gaussian_noise(noise_tensor, scale_factor):\n",
        "    batch_size, channels, height, width = noise_tensor.shape\n",
        "    scaled_noise = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            noise_np = noise_tensor[i, c].cpu().numpy()\n",
        "\n",
        "            # Resize the noise to emulate higher/lower resolution\n",
        "            scaled_noise_np = cv2.resize(noise_np, (int(width * scale_factor), int(height * scale_factor)), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Resize it back to the original size\n",
        "            scaled_noise_np = cv2.resize(scaled_noise_np, (width, height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Convert back to tensor\n",
        "            scaled_noise_tensor = torch.tensor(scaled_noise_np).to(noise_tensor.device)\n",
        "\n",
        "            # Append scaled noise to the list\n",
        "            if c == 0:\n",
        "                scaled_noise_batch = scaled_noise_tensor.unsqueeze(0)\n",
        "            else:\n",
        "                scaled_noise_batch = torch.cat((scaled_noise_batch, scaled_noise_tensor.unsqueeze(0)), 0)\n",
        "\n",
        "        scaled_noise.append(scaled_noise_batch.unsqueeze(0))\n",
        "\n",
        "    scaled_noise_tensor = torch.cat(scaled_noise, 0)\n",
        "    return scaled_noise_tensor\n",
        "\n",
        "# Define the function to apply rotational Gaussian noise with scaling\n",
        "def apply_rotational_gaussian_noise(image_tensor, std=40, mean=0, num_frames=50):\n",
        "    batch_size, channels, height, width = image_tensor.shape\n",
        "\n",
        "    # Randomly determine the center point once at the start\n",
        "    center_x = np.random.randint(0, width)\n",
        "    center_y = np.random.randint(0, height)\n",
        "    center = (center_x, center_y)\n",
        "\n",
        "    # Randomly vary the standard deviation at the start\n",
        "    std_variation = np.random.uniform(0.8, 2.0) * std\n",
        "\n",
        "    # Generate noise using torch.randn\n",
        "    noise_tensor = torch.randn_like(image_tensor) * std_variation + mean\n",
        "\n",
        "    # Randomly select a scale factor between 0.5 and 1.5\n",
        "    scale_factor = np.random.uniform(0.75, 2.0)\n",
        "\n",
        "    # Scale the Gaussian noise\n",
        "    scaled_noise_tensor = scale_gaussian_noise(noise_tensor, scale_factor)\n",
        "\n",
        "    noisy_images = []\n",
        "\n",
        "    rotation_factor = np.random.uniform(1, 3)\n",
        "    for _ in range(num_frames):\n",
        "\n",
        "        angle = np.random.uniform(0, rotation_factor)\n",
        "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "\n",
        "        # Apply rotation to each image in the batch\n",
        "        rotated_noise_batch = []\n",
        "        for i in range(batch_size):\n",
        "            noise_np = scaled_noise_tensor[i].permute(1, 2, 0).cpu().numpy()\n",
        "            rotated_noise_np = cv2.warpAffine(noise_np, rotation_matrix, (width, height))\n",
        "            rotated_noise_tensor = torch.tensor(rotated_noise_np).permute(2, 0, 1).to(image_tensor.device)\n",
        "            rotated_noise_batch.append(rotated_noise_tensor)\n",
        "\n",
        "        rotated_noise_batch = torch.stack(rotated_noise_batch)\n",
        "\n",
        "        # Add rotated noise to image\n",
        "        noisy_image_tensor = image_tensor + rotated_noise_batch / 255.0\n",
        "        noisy_images.append(noisy_image_tensor)\n",
        "\n",
        "    # Stack and average the noisy images\n",
        "    stacked_image_tensor = torch.stack(noisy_images).mean(dim=0)\n",
        "\n",
        "    # Clip values to the range [0, 1]\n",
        "    stacked_image_tensor = torch.clamp(stacked_image_tensor, 0, 1)\n",
        "\n",
        "    return stacked_image_tensor\n",
        "\n",
        "# Define the function to add combined noise\n",
        "def add_combined_noise(image, mean=0, std=0.075, noise_type=None, sp_prob=0.002):\n",
        "    # Randomly select a scale factor between 0.5 and 1.5\n",
        "    scale_factor = np.random.uniform(0.75, 2.5)\n",
        "\n",
        "    if noise_type == 'rotational':\n",
        "        noisy_image = apply_rotational_gaussian_noise(image, std=std*255, mean=mean*255)\n",
        "    elif noise_type == 'poisson':\n",
        "        multi = np.random.uniform(15, 25)\n",
        "        lambda_ = image * multi  # Adjust the multiplier to control intensity\n",
        "        poisson_noise = torch.poisson(lambda_) / multi - image\n",
        "\n",
        "        noisy_image = image + poisson_noise\n",
        "    else:\n",
        "        # Randomly vary the standard deviation at the start\n",
        "        std_variation = np.random.uniform(0.75, 1.75) * std\n",
        "\n",
        "        # Apply Gaussian noise\n",
        "        gaussian_noise = torch.randn(image.size(), device=image.device) * std_variation + mean\n",
        "\n",
        "        # Scale the Gaussian noise\n",
        "        gaussian_noise = scale_gaussian_noise(gaussian_noise, scale_factor)\n",
        "\n",
        "        noisy_image = image + gaussian_noise\n",
        "\n",
        "        # Clamp values to ensure non-negativity\n",
        "        noisy_image = torch.clamp(noisy_image, 0., None)\n",
        "\n",
        "        # Optionally apply other types of noise\n",
        "        if noise_type == 'salt_and_pepper':\n",
        "            sp_noise = torch.rand(image.size(), device=image.device)\n",
        "            salt_pepper_noise = torch.where(sp_noise < sp_prob / 2, torch.ones_like(image), torch.zeros_like(image))\n",
        "            salt_pepper_noise = torch.where(sp_noise > 1 - sp_prob / 2, -torch.ones_like(image), salt_pepper_noise)\n",
        "\n",
        "            # Scale the salt and pepper noise\n",
        "            salt_pepper_noise = scale_gaussian_noise(salt_pepper_noise, scale_factor)\n",
        "\n",
        "            noisy_image += salt_pepper_noise\n",
        "\n",
        "    noisy_image = torch.clamp(noisy_image, 0., 1.)\n",
        "    return noisy_image\n",
        "# Function to visualize images\n",
        "def visualize_images(original, noisy_images, titles, figsize=(20, 10)):\n",
        "    num_images = len(noisy_images) + 1\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(original.permute(1, 2, 0).cpu().numpy())\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Noisy images\n",
        "    for i, noisy_image in enumerate(noisy_images):\n",
        "        axes[i+1].imshow(noisy_image.permute(1, 2, 0).cpu().numpy())\n",
        "        axes[i+1].set_title(titles[i])\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Load an example image and convert it to a tensor\n",
        "image_path = '/notebooks/astro9.jpg'  # Update this path to your image file\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "image_tensor = transform(image).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generate noisy images\n",
        "noisy_image_gaussian = add_combined_noise(image_tensor, noise_type=None)\n",
        "noisy_image_rotational = add_combined_noise(image_tensor, noise_type='rotational')\n",
        "noisy_image_poisson = add_combined_noise(image_tensor, noise_type='poisson')\n",
        "noisy_image_SP = add_combined_noise(image_tensor, noise_type='salt_and_pepper')\n",
        "\n",
        "# Visualize images\n",
        "visualize_images(\n",
        "    image_tensor.squeeze(0),\n",
        "    [\n",
        "        noisy_image_gaussian.squeeze(0),\n",
        "        noisy_image_rotational.squeeze(0),\n",
        "        noisy_image_poisson.squeeze(0),\n",
        "        noisy_image_SP.squeeze(0)\n",
        "    ],\n",
        "    [\"Gaussian Noise\",  \"Rotational Gaussian Noise\", \"poisson\", \"SP\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training loop/training"
      ],
      "metadata": {
        "id": "I-QMYJVZIElS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-29T23:23:19.315090Z",
          "iopub.status.busy": "2024-05-29T23:23:19.314642Z"
        },
        "id": "LHbRnK0YBvYW"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Initialize the accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Disable LaTeX rendering in matplotlib\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.05, contrast=0.15, saturation=0.15, hue=0.05),\n",
        "])\n",
        "\n",
        "# Function to load checkpoints\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        start_step = checkpoint['step']\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path} (Epoch: {start_epoch}, Step: {start_step})\")\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        start_step = 0\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    return model, optimizer, start_epoch, start_step\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = ImprovedDenoisingNetwork(in_channels=3, out_channels=3)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=6e-5, weight_decay=0.1)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "hybrid_loss = CustomLoss()\n",
        "\n",
        "# Prepare everything with the accelerator\n",
        "model, optimizer, hybrid_loss, train_loader, val_loader = accelerator.prepare(model, optimizer, hybrid_loss, train_loader, val_loader)\n",
        "\n",
        "\n",
        "# Directory for saving model checkpoints\n",
        "model_save_dir = \"saved_models\"\n",
        "checkpoint_path = os.path.join(model_save_dir, \"/notebooks/saved_models/latest_checkpoint_12_step_2500.pth\")\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(model_save_dir):\n",
        "    os.makedirs(model_save_dir)\n",
        "\n",
        "# Load model and optimizer state if checkpoint exists\n",
        "model, optimizer, start_epoch, start_step = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 150\n",
        "print_every = 250  # Print progress every 250 steps\n",
        "save_every = 1500   # Save progress every 1500 steps\n",
        "\n",
        "def save_model(epoch, step, model, optimizer, model_save_dir):\n",
        "    model_path = os.path.join(model_save_dir, f\"latest_checkpoint_{epoch+1}_step_{step+1}.pth\")\n",
        "    accelerator.save({\n",
        "        'epoch': epoch,\n",
        "        'step': step,\n",
        "        'model_state_dict': accelerator.get_state_dict(model),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, model_path)\n",
        "    print(f\"Model saved at {model_path}\")\n",
        "\n",
        "def save_progress_images(model, images, noisy_images, epoch, step, save_dir=\"denoising_progress\"):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        original = images[0].unsqueeze(0)\n",
        "        noisy = noisy_images[0].unsqueeze(0)\n",
        "        denoised = model(noisy).squeeze(0).cpu()\n",
        "        original = original.squeeze(0).cpu()\n",
        "        noisy = noisy.squeeze(0).cpu()\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        axes[0].imshow(TF.to_pil_image(original))\n",
        "        axes[0].set_title(\"Original\")\n",
        "        axes[1].imshow(TF.to_pil_image(noisy))\n",
        "        axes[1].set_title(\"Noisy\")\n",
        "        axes[2].imshow(TF.to_pil_image(denoised))\n",
        "        axes[2].set_title(f\"Denoised - Epoch {epoch+1}, Step {step+1}\")\n",
        "        plt.savefig(os.path.join(save_dir, f\"epoch_{epoch+1}_step_{step+1}.png\"))\n",
        "        plt.close()\n",
        "    model.train()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    noise_types = [None, 'rotational', 'salt_and_pepper', 'poisson']\n",
        "\n",
        "    for step, inputs in enumerate(train_loader):\n",
        "        if epoch == start_epoch and step < start_step:\n",
        "            continue  # Skip the steps already covered in the previous checkpoint\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            inputs = inputs.to(accelerator.device)\n",
        "            augmented_inputs = augmentation_transform(inputs)\n",
        "            selected_noise = random.choice(noise_types)\n",
        "            noisy_inputs = add_combined_noise(augmented_inputs, noise_type=selected_noise)\n",
        "            with accelerator.autocast():\n",
        "                outputs = model(noisy_inputs)\n",
        "                loss = hybrid_loss(outputs, augmented_inputs, noisy_inputs)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss detected at Epoch {epoch+1}, Step {step+1}\")\n",
        "                break\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if (step + 1) % print_every == 0:\n",
        "                print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            if (step + 1) % save_every == 0:\n",
        "                save_progress_images(model, augmented_inputs, noisy_inputs, epoch, step)\n",
        "\n",
        "            if (step + 1) % 2500 == 0:\n",
        "                save_model(epoch, step, model, optimizer, model_save_dir)\n",
        "                print(f\"Model saved at epoch {epoch+1}, step {step+1}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "    scheduler.step(epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model denoising"
      ],
      "metadata": {
        "id": "Q99eDUSsIalW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-28T21:24:04.211689Z",
          "iopub.status.busy": "2024-05-28T21:24:04.211100Z",
          "iopub.status.idle": "2024-05-28T21:24:05.769789Z",
          "shell.execute_reply": "2024-05-28T21:24:05.769103Z",
          "shell.execute_reply.started": "2024-05-28T21:24:04.211659Z"
        },
        "id": "YhjdKIxtGVQ_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model (make sure to replace 'your_model_file.pth' with the path to your model file)\n",
        "checkpoint = torch.load('/notebooks/saved_models/latest_checkpoint_8_step_2500.pth', map_location=device)\n",
        "model = ImprovedDenoisingNetwork(in_channels=3, out_channels=3)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Function to load an image\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return image\n",
        "\n",
        "# Function to split image into overlapping tiles\n",
        "def image_to_tiles(image, tile_size, overlap):\n",
        "    w, h = image.size\n",
        "    step = tile_size - overlap\n",
        "    tiles = []\n",
        "    positions = []\n",
        "    for i in range(0, h, step):\n",
        "        for j in range(0, w, step):\n",
        "            right = min(j + tile_size, w)\n",
        "            bottom = min(i + tile_size, h)\n",
        "            tile = image.crop((j, i, right, bottom))\n",
        "            tiles.append(tile)\n",
        "            positions.append((i, j, right - j, bottom - i))\n",
        "    return tiles, positions\n",
        "\n",
        "# Create an alpha mask for blending\n",
        "def create_alpha_mask(tile_size, overlap):\n",
        "    mask = np.ones((tile_size, tile_size), dtype=np.float32)\n",
        "    ramp = np.linspace(0, 1, overlap)\n",
        "    mask[:overlap, :] *= ramp[:, None]\n",
        "    mask[-overlap:, :] *= ramp[::-1, None]\n",
        "    mask[:, :overlap] *= ramp[None, :]\n",
        "    mask[:, -overlap:] *= ramp[None, ::-1]\n",
        "    return mask\n",
        "\n",
        "# Function to merge tiles back to image with alpha blending\n",
        "def tiles_to_image(tiles, positions, image_size, tile_size, overlap):\n",
        "    full_image = np.zeros((image_size[1], image_size[0], 3), dtype=np.float32)\n",
        "    alpha_map = np.zeros((image_size[1], image_size[0], 3), dtype=np.float32)\n",
        "    alpha_mask = create_alpha_mask(tile_size, overlap)\n",
        "\n",
        "    for idx, (i, j, width, height) in enumerate(positions):\n",
        "        tile = np.array(tiles[idx])[:height, :width]  # Crop tile to original size before padding\n",
        "        h, w, _ = tile.shape\n",
        "\n",
        "        # Ensure the alpha mask matches the tile size\n",
        "        mask = alpha_mask[:h, :w, np.newaxis]\n",
        "\n",
        "        full_image[i:i+height, j:j+width] += tile * mask\n",
        "        alpha_map[i:i+height, j:j+width] += mask\n",
        "\n",
        "    final_image = full_image / np.maximum(alpha_map, 1e-8)  # Normalize by the alpha map, avoiding division by zero\n",
        "    final_image = np.clip(final_image, 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(final_image)\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load and process the image in overlapping tiles\n",
        "image_path = 'astro9.jpg'  # Replace with your image path\n",
        "original_image = load_image(image_path)\n",
        "tile_size = 256  # Define tile size to match your model's expected input size\n",
        "overlap = 32     # Define overlap size\n",
        "tiles, positions = image_to_tiles(original_image, tile_size, overlap)\n",
        "\n",
        "# Process each tile\n",
        "processed_tiles = []\n",
        "for tile in tiles:\n",
        "    input_tensor = transform(tile).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        processed_tile = model(input_tensor).squeeze(0)\n",
        "\n",
        "    processed_tile = torch.clamp(processed_tile, 0, 1)  # Ensure output is in correct range\n",
        "\n",
        "    processed_tiles.append(transforms.ToPILImage()(processed_tile.cpu()))\n",
        "\n",
        "# Reconstruct the image from tiles\n",
        "reconstructed_image = tiles_to_image(processed_tiles, positions, original_image.size, tile_size, overlap)\n",
        "\n",
        "# Function to save the denoised image\n",
        "def save_image(image, path):\n",
        "    image.save(path)\n",
        "    print(f\"Image saved at {path}\")\n",
        "\n",
        "# Function to display images\n",
        "def show_images(original, reconstructed):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axes[0].imshow(np.asarray(original))\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(np.asarray(reconstructed))\n",
        "    axes[1].set_title('Reconstructed Image')\n",
        "    axes[1].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Display the images\n",
        "show_images(original_image, reconstructed_image)\n",
        "\n",
        "# Save the denoised image\n",
        "output_path = 'denoised_astro9.png'  # Replace with your desired output path\n",
        "save_image(reconstructed_image, output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5048981,
          "sourceId": 8468467,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}