{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Change runtime type to T4 GPU for faster processing.**"
      ],
      "metadata": {
        "id": "NgEXfN2hv2jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Run this cell first to Download Model/Install dependencies, may take a few minutes to finish..."
      ],
      "metadata": {
        "id": "IZBhX7iUQzAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install timm pytorch_wavelets\n",
        "!gdown --id '1MM8fZ-TFhWJYCwUnL8oI2KFDiNuV_x0K' -O 'latest_checkpoint.pth'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nhAA8LW7ROyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Run this cell next, for model initialization.."
      ],
      "metadata": {
        "id": "yp2YD5Z-Re-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mSwiWNGVQnso"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_\n",
        "from timm.models.vision_transformer import Block\n",
        "import pytorch_wavelets\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers=4):\n",
        "        super(ResidualDenseBlock, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_ch = in_channels + i * growth_rate\n",
        "            out_ch = growth_rate if i < num_layers - 1 else in_channels\n",
        "            self.layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = [x]\n",
        "        for layer in self.layers:\n",
        "            out = self.relu(layer(torch.cat(inputs, dim=1)))\n",
        "            inputs.append(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, width, height = x.size()\n",
        "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, channels, width, height)\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(CBAMBlock, self).__init__()\n",
        "        self.channel_attention = ChannelAttentionBlock(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttentionBlock()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.channel_attention(x)\n",
        "        x_out = self.spatial_attention(x_out)\n",
        "        return x_out\n",
        "\n",
        "class ChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttentionBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.fc(y)\n",
        "        return x * y\n",
        "\n",
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttentionBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_out = torch.cat([avg_out, max_out], dim=1)\n",
        "        x_out = self.conv(x_out)\n",
        "        return x * self.sigmoid(x_out)\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_blocks=3, num_layers=4):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.blocks = nn.ModuleList([ResidualDenseBlock(in_channels, growth_rate, num_layers) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class DynamicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(DynamicConv2d, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding)\n",
        "\n",
        "class WaveletTransform(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveletTransform, self).__init__()\n",
        "        self.dwt = pytorch_wavelets.DWTForward(J=1, wave='haar', mode='zero')\n",
        "        self.iwt = pytorch_wavelets.DWTInverse(wave='haar', mode='zero')\n",
        "\n",
        "    def forward(self, x):\n",
        "        yl, yh = self.dwt(x)\n",
        "        recon = self.iwt((yl, yh))\n",
        "        return recon\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.inter_channels = in_channels // 2\n",
        "\n",
        "        self.g = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.theta = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.phi = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.W = nn.Conv2d(self.inter_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        nn.init.constant_(self.W.weight, 0)\n",
        "        nn.init.constant_(self.W.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "\n",
        "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
        "        g_x = g_x.permute(0, 2, 1)\n",
        "\n",
        "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
        "        theta_x = theta_x.permute(0, 2, 1)\n",
        "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
        "\n",
        "        f = torch.matmul(theta_x, phi_x)\n",
        "        f_div_C = F.softmax(f, dim=-1)\n",
        "\n",
        "        y = torch.matmul(f_div_C, g_x)\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "        y = y.view(batch_size, self.inter_channels, h, w)\n",
        "        W_y = self.W(y)\n",
        "        z = W_y + x\n",
        "\n",
        "        return z\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(self._make_layer(in_channels + i * growth_rate, growth_rate))\n",
        "\n",
        "    def _make_layer(self, in_channels, growth_rate):\n",
        "        layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(growth_rate),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            new_features = layer(torch.cat(features, dim=1))\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "class PixelShuffleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, upscale_factor):\n",
        "        super(PixelShuffleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class MultiScaleFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(MultiScaleFeatureExtractor, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=1, padding=3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.non_local = NonLocalBlock(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.relu(self.bn1(self.conv1(x)))\n",
        "        out2 = self.relu(self.bn2(self.conv2(x)))\n",
        "        out3 = self.relu(self.bn3(self.conv3(x)))\n",
        "        multi_scale_features = out1 + out2 + out3\n",
        "        return self.non_local(multi_scale_features)\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, depths, num_heads, window_size=7):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depths = depths\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio=4., qkv_bias=True)\n",
        "            for _ in range(depths)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (batch_size, num_patches, embed_dim)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, channels, height, width)\n",
        "        return x\n",
        "\n",
        "class ImprovedDenoisingNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ImprovedDenoisingNetwork, self).__init__()\n",
        "        self.initial_conv = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            DenseBlock(64, 32, num_layers=4),\n",
        "            ResidualBlock(192, 128, stride=2),  # DenseBlock output channels + 64 initial channels\n",
        "            ResidualBlock(128, 128, stride=2)\n",
        "        )\n",
        "\n",
        "        self.multi_scale = MultiScaleFeatureExtractor(128)\n",
        "        self.transformer = SwinTransformerBlock(embed_dim=128, depths=2, num_heads=4)\n",
        "\n",
        "        self.rir_block1 = ResidualInResidualDenseBlock(128, 32, num_blocks=2, num_layers=3)\n",
        "        self.attention1 = CBAMBlock(128)\n",
        "\n",
        "        self.rir_block2 = ResidualInResidualDenseBlock(128, 32, num_blocks=2, num_layers=3)\n",
        "        self.attention2 = CBAMBlock(128)\n",
        "\n",
        "        self.dynamic_conv = DynamicConv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.wavelet_transform = WaveletTransform()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            PixelShuffleBlock(128, 64, upscale_factor=2),\n",
        "            PixelShuffleBlock(64, 64, upscale_factor=2),\n",
        "            nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        encoded = self.encoder(x)\n",
        "        multi_scale_features = self.multi_scale(encoded)\n",
        "        transformer_features = self.transformer(multi_scale_features)\n",
        "\n",
        "        rir1 = self.rir_block1(transformer_features)\n",
        "        att1 = self.attention1(rir1)\n",
        "\n",
        "        rir2 = self.rir_block2(att1)\n",
        "        att2 = self.attention2(rir2)\n",
        "\n",
        "        dynamic_conv_features = self.dynamic_conv(att2)\n",
        "        wavelet_features = self.wavelet_transform(dynamic_conv_features)\n",
        "\n",
        "        decoded = self.decoder(wavelet_features)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Finally run this cell and select a file to denoise, the denoised image will be Saved to the colab file directories \"denoised\" folder."
      ],
      "metadata": {
        "id": "KR5dCMmliG3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload the image\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the image path\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model (make sure to replace 'your_model_file.pth' with the path to your model file)\n",
        "checkpoint = torch.load('latest_checkpoint.pth', map_location=device)\n",
        "model = ImprovedDenoisingNetwork(in_channels=3, out_channels=3)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to load an image\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return image\n",
        "\n",
        "# Function to split image into overlapping tiles\n",
        "def image_to_tiles(image, tile_size, overlap):\n",
        "    w, h = image.size\n",
        "    step = tile_size - overlap\n",
        "    tiles = []\n",
        "    positions = []\n",
        "    for i in range(0, h, step):\n",
        "        for j in range(0, w, step):\n",
        "            right = min(j + tile_size, w)\n",
        "            bottom = min(i + tile_size, h)\n",
        "            tile = image.crop((j, i, right, bottom))\n",
        "            tiles.append(tile)\n",
        "            positions.append((i, j, right - j, bottom - i))\n",
        "    return tiles, positions\n",
        "\n",
        "# Create an alpha mask for blending\n",
        "def create_alpha_mask(tile_size, overlap):\n",
        "    mask = np.ones((tile_size, tile_size), dtype=np.float32)\n",
        "    ramp = np.linspace(0, 1, overlap)\n",
        "    mask[:overlap, :] *= ramp[:, None]\n",
        "    mask[-overlap:, :] *= ramp[::-1, None]\n",
        "    mask[:, :overlap] *= ramp[None, :]\n",
        "    mask[:, -overlap:] *= ramp[None, ::-1]\n",
        "    return mask\n",
        "\n",
        "# Function to merge tiles back to image with alpha blending\n",
        "def tiles_to_image(tiles, positions, image_size, tile_size, overlap):\n",
        "    full_image = np.zeros((image_size[1], image_size[0], 3), dtype=np.float32)\n",
        "    alpha_map = np.zeros((image_size[1], image_size[0], 3), dtype=np.float32)\n",
        "    alpha_mask = create_alpha_mask(tile_size, overlap)\n",
        "\n",
        "    for idx, (i, j, width, height) in enumerate(positions):\n",
        "        tile = np.array(tiles[idx])[:height, :width]  # Crop tile to original size before padding\n",
        "        h, w, _ = tile.shape\n",
        "\n",
        "        # Ensure the alpha mask matches the tile size\n",
        "        mask = alpha_mask[:h, :w, np.newaxis]\n",
        "\n",
        "        full_image[i:i+height, j:j+width] += tile * mask\n",
        "        alpha_map[i:i+height, j:j+width] += mask\n",
        "\n",
        "    final_image = full_image / np.maximum(alpha_map, 1e-8)  # Normalize by the alpha map, avoiding division by zero\n",
        "    final_image = np.clip(final_image, 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(final_image)\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load and process the image in overlapping tiles\n",
        "original_image = load_image(image_path)\n",
        "tile_size = 256  # Define tile size to match your model's expected input size\n",
        "overlap = 32     # Increased overlap size for better blending\n",
        "tiles, positions = image_to_tiles(original_image, tile_size, overlap)\n",
        "\n",
        "# Process each tile\n",
        "processed_tiles = []\n",
        "for tile in tiles:\n",
        "    input_tensor = transform(tile).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        processed_tile = model(input_tensor).squeeze(0)\n",
        "\n",
        "    processed_tile = torch.clamp(processed_tile, 0, 1)  # Ensure output is in correct range\n",
        "\n",
        "    processed_tiles.append(transforms.ToPILImage()(processed_tile.cpu()))\n",
        "\n",
        "# Reconstruct the image from tiles\n",
        "reconstructed_image = tiles_to_image(processed_tiles, positions, original_image.size, tile_size, overlap)\n",
        "\n",
        "# Function to save the denoised image\n",
        "def save_image(image, path):\n",
        "  #create path\n",
        "  if not os.path.exists(os.path.dirname(path)):\n",
        "    os.makedirs(os.path.dirname(path))\n",
        "  image.save(path)\n",
        "  print(f\"Image saved at {path}\")\n",
        "\n",
        "# Function to display images\n",
        "def show_images(original, reconstructed):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axes[0].imshow(np.asarray(original))\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(np.asarray(reconstructed))\n",
        "    axes[1].set_title('Reconstructed Image')\n",
        "    axes[1].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the images\n",
        "show_images(original_image, reconstructed_image)\n",
        "\n",
        "# Save the denoised image\n",
        "output_path = os.path.join('/content/denoised', 'denoised_' + image_path)  # Replace with your desired output path\n",
        "save_image(reconstructed_image, output_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lVFDK99sQ9mE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}